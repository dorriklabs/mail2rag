# ============================================================================
# MAIL2RAG3 - Environment Variables
# ============================================================================

# ============================================================================
# EMAIL (IMAP / SMTP)
# ============================================================================

# IMAP (réception des emails)
IMAP_SERVER=imap.example.com
IMAP_PORT=993
IMAP_USER=rag@example.com
IMAP_PASSWORD=CHANGE_ME
IMAP_TIMEOUT=30

# SMTP (envoi des réponses)
SMTP_SERVER=smtp.example.com
SMTP_PORT=587
SMTP_USER=rag@example.com
SMTP_PASSWORD=CHANGE_ME
SMTP_TIMEOUT=30

# Dossier IMAP à surveiller et critère de recherche
IMAP_FOLDER=INBOX
IMAP_SEARCH_CRITERIA=UNSEEN
IMAP_POLL_INTERVAL=60

# ============================================================================
# LM STUDIO / VISION AI
# ============================================================================

# LLM Chat / Vision (LM Studio local)
AI_API_URL=http://host.docker.internal:1234/v1/chat/completions
AI_API_KEY=lm-studio
AI_MODEL_NAME=qwen/qwen3-vl-8b

# Embedding model
EMBEDDING_MODEL_PREF=text-embedding-bge-m3

VISION_ENABLE_IMAGES=true
VISION_ENABLE_PDF=true
VISION_TEMPERATURE=0.0
VISION_MAX_TOKENS=1500
VISION_TIMEOUT=90

# LLM CHAT (génération de réponses RAG)
# Utilise le même modèle que Vision pour économiser la VRAM (8Go)
LLM_CHAT_MODEL=qwen/qwen3-vl-8b
LLM_CHAT_TIMEOUT=120

# Limite de tokens pour le contexte RAG (évite les dépassements de context window)
# Le système coupe automatiquement les chunks pour rester sous cette limite.
# Configurez cette valeur ~75% du contexte configuré dans LM Studio :
#   - LM Studio 4K  → LLM_MAX_CONTEXT_TOKENS=3000
#   - LM Studio 8K  → LLM_MAX_CONTEXT_TOKENS=6000
#   - LM Studio 16K → LLM_MAX_CONTEXT_TOKENS=12000
# Note: L'API LM Studio ne permet pas de récupérer cette valeur automatiquement.
LLM_MAX_CONTEXT_TOKENS=6000

# Prompt système pour le chat RAG (personnalisable)
# Ce prompt définit le comportement du LLM pour répondre aux questions
LLM_CHAT_SYSTEM_PROMPT=Tu es un assistant IA serviable. Réponds de manière concise et précise en te basant uniquement sur le contexte fourni.

# ============================================================================
# TIKA (DOCUMENT TEXT EXTRACTION)
# ============================================================================

TIKA_SERVER_URL=http://tika:9998
TIKA_TIMEOUT=180
TIKA_ENABLE=true
TIKA_FALLBACK_TO_VISION=true

# ============================================================================
# PROMPTS
# ============================================================================

PROMPTS_DIR=/etc/mail2rag/prompts
VISION_AI_PROMPT_FILE=vision_ai.txt
SUPPORT_QA_PROMPT_FILE=support_qa_prompt.txt

# ============================================================================
# CHEMINS / ARCHIVE / ROUTING
# ============================================================================

STATE_PATH=/var/lib/mail2rag/state.json
ARCHIVE_PATH=/var/lib/mail2rag/mail2rag_archive
ROUTING_PATH=/etc/mail2rag/routing.json

ARCHIVE_BASE_URL=http://localhost:8080

# ============================================================================
# LOGGING
# ============================================================================

LOG_LEVEL=INFO
# 10 MB
LOG_MAX_BYTES=10485760
LOG_BACKUP_COUNT=5

LOG_TRUNCATE_HEAD=5
LOG_TRUNCATE_TAIL=3
LOG_MAX_LINE_LENGTH=500

# ============================================================================
# DEFAULTS / LIMITES / FILTRAGE
# ============================================================================

DEFAULT_WORKSPACE=default-workspace
DEFAULT_SUBJECT=No_Subject
MAX_FILENAME_LENGTH=100

# Extensions autorisées (formats supportés par Tika Full)
ALLOWED_EXTENSIONS=.pdf,.docx,.doc,.txt,.md,.csv,.xlsx,.xls,.pptx,.ppt,.html,.xml,.json,.jpg,.jpeg,.png,.bmp,.webp,.odt,.ods,.odp,.rtf,.epub,.eml,.msg,.gif,.tiff,.tif,.svg

# Extensions bloquées
BLOCKED_EXTENSIONS=.exe,.bin,.bat,.sh,.zip,.rar,.7z,.tar,.gz,.iso,.dll

# Taille min image (KB) pour éviter les logos / signatures
MIN_IMAGE_SIZE_KB=5

# DPI pour conversion PDF vers image (Vision AI)
OCR_DPI=300

# ============================================================================
# LLM SETTINGS (RÉPONSES)
# ============================================================================

# Température pour les fonctions LLM dans mail2rag (résumés, support Q/A)
# 0.0 = déterministe, 0.7 = légèrement créatif (recommandé pour résumés)
# Note: LLM_CHAT_TEMPERATURE (RAG Proxy) reste à 0.1 pour des réponses factuelles
DEFAULT_LLM_TEMPERATURE=0.7
DEFAULT_REFUSAL_RESPONSE=Je ne trouve pas d information pertinente dans vos documents pour répondre à cette question.

# ============================================================================
# WORKERS / CONCURRENCE
# ============================================================================

WORKER_COUNT=1
WORKER_QUEUE_SIZE=50

# ============================================================================
# EMAIL SUMMARY (RÉSUMÉS D’INGESTION)
# ============================================================================

ENABLE_EMAIL_SUMMARY=false
SUMMARY_MAX_SENTENCES=3
SUMMARY_MAX_TOKENS=150

# ============================================================================
# SUPPORT QA (réécriture Q/R)
# ============================================================================

SUPPORT_QA_TEMPERATURE=0.1
SUPPORT_QA_MAX_TOKENS=1200

# ============================================================================
# RAG PROXY / HYBRID SEARCH
# ============================================================================

RAG_PROXY_URL=http://rag_proxy:8000
USE_RAG_PROXY_FOR_SEARCH=true

AUTO_REBUILD_BM25=true

VECTOR_DB_HOST=qdrant
VECTOR_DB_PORT=6333
VECTOR_DB_COLLECTION=documents

# LM Studio URL pour RAG Proxy (embeddings + chat)
LM_STUDIO_URL=http://host.docker.internal:1234

# Authentification API (optionnel, désactivé par défaut)
RAG_PROXY_API_KEY=
API_KEY_ENABLED=false

# Mode multi-collections (true pour auto-détection des workspaces)
MULTI_COLLECTION_MODE=true

# Reranking local (cross-encoder)
USE_LOCAL_RERANKER=true
LOCAL_RERANKER_MODEL=cross-encoder/ms-marco-MiniLM-L-6-v2

# Modèle d'embedding pour RAG Proxy
EMBED_MODEL=text-embedding-bge-m3

# ============================================================================
# MAINTENANCE / SYNC
# ============================================================================

# Nettoyer l'archive avant la synchronisation au démarrage
CLEANUP_ARCHIVE_BEFORE_SYNC=false

# Synchroniser l'archive au démarrage
SYNC_ON_START=true

# ============================================================================
# CHAT HISTORY
# ============================================================================

# Sauvegarder l'historique de chat dans l'archive (log seulement, non indexé)
SAVE_CHAT_HISTORY=true

# ============================================================================
# CHUNKING INTELLIGENT
# ============================================================================
CHUNK_SIZE=800
CHUNK_OVERLAP=100
CHUNKING_STRATEGY=recursive
